# UDFs in Scala to use in pySpark

Just testing some basic UDFs to test how it works. Nothing fancy.

At the moment addOne is working.

Reference sites:

* https://stackoverflow.com/questions/33233737/spark-how-to-map-python-with-scala-or-java-user-defined-functions
* https://medium.com/wbaa/using-scala-udfs-in-pyspark-b70033dd69b9

* https://www.crowdstrike.com/blog/spark-hot-potato-passing-dataframes-between-scala-spark-and-pyspark/

Others related links:
* https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-udfs.html
* https://medium.com/@QuantumBlack/spark-udf-deep-insights-in-performance-f0a95a4d8c62
* https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html
* https://medium.com/@mrpowers/spark-user-defined-functions-udfs-6c849e39443b
* http://biercoff.com/easily-measuring-code-execution-time-in-scala/
